{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD RAW Text Seq and Init Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "\n",
    "raw_data_path = '/workspace/pj/data/corpus/raw_corpus_bpe.txt'\n",
    "# raw_data_path = '/workspace/data/corpus/first_5_lines_bpe.txt'\n",
    "\n",
    "vocab_path = '/workspace/pj/data/vocabs/chord.json'\n",
    "with open(vocab_path, 'r') as file:\n",
    "    vocab = json.load(file)\n",
    "\n",
    "print(\"Initial Vocabulary:\", vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = []\n",
    "with open(raw_data_path, 'r') as f:\n",
    "    for line in tqdm(f, desc=\"reading original txt file...\"):\n",
    "        raw_data.append(line.strip())\n",
    "\n",
    "\n",
    "def extract_chord_seq_list(toks, ratio=4):\n",
    "    if isinstance(toks, str):\n",
    "        toks = toks.split()\n",
    "\n",
    "    l_toks = len(toks)\n",
    "\n",
    "    chord_list = []\n",
    "    \n",
    "    for idx in range(0, l_toks, ratio):\n",
    "        t1, t2, t3, t4 = toks[idx : idx + 4]\n",
    "        \n",
    "        if t1[0] == 'h' or t1[0] == 'H':\n",
    "            chord_list.append(t1)\n",
    "            \n",
    "    return chord_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chord sequnece만 추출\n",
    "\n",
    "chord_seq = []\n",
    "cnt = 0\n",
    "for data in tqdm(raw_data):\n",
    "    if cnt == 2000:\n",
    "        break\n",
    "    out_list = extract_chord_seq_list(data)\n",
    "    chord_seq.append(out_list)\n",
    "    cnt += 1\n",
    "print(len(chord_seq))\n",
    "print(chord_seq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Vocab & Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_dict = {\"<eos>\": 1,\n",
    "    \"<pad>\": 0,\n",
    "    \"<bos>\": 2,\n",
    "    \"HC+\": 134,\n",
    "    \"HC/o7\": 135,\n",
    "    \"HCD7\": 3,\n",
    "    \"HCM\": 4,\n",
    "    \"HCM7\": 5,\n",
    "    \"HCm\": 6,\n",
    "    \"HCm7\": 7,\n",
    "    \"HCo\": 8,\n",
    "    \"HCo7\": 9,\n",
    "    \"HCsus2\": 10,\n",
    "    \"HCsus4\": 11,\n",
    "    \"Hd+\": 12,\n",
    "    \"Hd/o7\": 13,\n",
    "    \"HdD7\": 14,\n",
    "    \"HdM\": 15,\n",
    "    \"HdM7\": 16,\n",
    "    \"Hdm\": 17,\n",
    "    \"Hdm7\": 18,\n",
    "    \"Hdo\": 19,\n",
    "    \"Hdo7\": 20,\n",
    "    \"Hdsus2\": 21,\n",
    "    \"Hdsus4\": 22,\n",
    "    \"HD+\": 23,\n",
    "    \"HD/o7\": 24,\n",
    "    \"HDD7\": 25,\n",
    "    \"HDM\": 26,\n",
    "    \"HDM7\": 27,\n",
    "    \"HDm\": 28,\n",
    "    \"HDm7\": 29,\n",
    "    \"HDo\": 30,\n",
    "    \"HDo7\": 31,\n",
    "    \"HDsus2\": 32,\n",
    "    \"HDsus4\": 33,\n",
    "    \"He+\": 34,\n",
    "    \"He/o7\": 35,\n",
    "    \"HeD7\": 36,\n",
    "    \"HeM\": 37,\n",
    "    \"HeM7\": 38,\n",
    "    \"Hem\": 39,\n",
    "    \"Hem7\": 40,\n",
    "    \"Heo\": 41,\n",
    "    \"Heo7\": 42,\n",
    "    \"Hesus2\": 43,\n",
    "    \"Hesus4\": 44,\n",
    "    \"HE+\": 45,\n",
    "    \"HE/o7\": 46,\n",
    "    \"HED7\": 47,\n",
    "    \"HEM\": 48,\n",
    "    \"HEM7\": 49,\n",
    "    \"HEm\": 50,\n",
    "    \"HEm7\": 51,\n",
    "    \"HEo\": 52,\n",
    "    \"HEo7\": 53,\n",
    "    \"HEsus2\": 54,\n",
    "    \"HEsus4\": 55,\n",
    "    \"HF+\": 56,\n",
    "    \"HF/o7\": 57,\n",
    "    \"HFD7\": 58,\n",
    "    \"HFM\": 59,\n",
    "    \"HFM7\": 60,\n",
    "    \"HFm\": 61,\n",
    "    \"HFm7\": 62,\n",
    "    \"HFo\": 63,\n",
    "    \"HFo7\": 64,\n",
    "    \"HFsus2\": 65,\n",
    "    \"HFsus4\": 66,\n",
    "    \"Hg+\": 67,\n",
    "    \"Hg/o7\": 68,\n",
    "    \"HgD7\": 69,\n",
    "    \"HgM\": 70,\n",
    "    \"HgM7\": 71,\n",
    "    \"Hgm\": 72,\n",
    "    \"Hgm7\": 73,\n",
    "    \"Hgo\": 74,\n",
    "    \"Hgo7\": 75,\n",
    "    \"Hgsus2\": 76,\n",
    "    \"Hgsus4\": 77,\n",
    "    \"HG+\": 78,\n",
    "    \"HG/o7\": 79,\n",
    "    \"HGD7\": 80,\n",
    "    \"HGM\": 81,\n",
    "    \"HGM7\": 82,\n",
    "    \"HGm\": 83,\n",
    "    \"HGm7\": 84,\n",
    "    \"HGo\": 85,\n",
    "    \"HGo7\": 86,\n",
    "    \"HGsus2\": 87,\n",
    "    \"HGsus4\": 88,\n",
    "    \"Ha+\": 89,\n",
    "    \"Ha/o7\": 90,\n",
    "    \"HaD7\": 91,\n",
    "    \"HaM\": 92,\n",
    "    \"HaM7\": 93,\n",
    "    \"Ham\": 94,\n",
    "    \"Ham7\": 95,\n",
    "    \"Hao\": 96,\n",
    "    \"Hao7\": 97,\n",
    "    \"Hasus2\": 98,\n",
    "    \"Hasus4\": 99,\n",
    "    \"HA+\": 100,\n",
    "    \"HA/o7\": 101,\n",
    "    \"HAD7\": 102,\n",
    "    \"HAM\": 103,\n",
    "    \"HAM7\": 104,\n",
    "    \"HAm\": 105,\n",
    "    \"HAm7\": 106,\n",
    "    \"HAo\": 107,\n",
    "    \"HAo7\": 108,\n",
    "    \"HAsus2\": 109,\n",
    "    \"HAsus4\": 110,\n",
    "    \"Hb+\": 111,\n",
    "    \"Hb/o7\": 112,\n",
    "    \"HbD7\": 113,\n",
    "    \"HbM\": 114,\n",
    "    \"HbM7\": 115,\n",
    "    \"Hbm\": 116,\n",
    "    \"Hbm7\": 117,\n",
    "    \"Hbo\": 118,\n",
    "    \"Hbo7\": 119,\n",
    "    \"Hbsus2\": 120,\n",
    "    \"Hbsus4\": 121,\n",
    "    \"HB+\": 122,\n",
    "    \"HB/o7\": 123,\n",
    "    \"HBD7\": 124,\n",
    "    \"HBM\": 125,\n",
    "    \"HBM7\": 126,\n",
    "    \"HBm\": 127,\n",
    "    \"HBm7\": 128,\n",
    "    \"HBo\": 129,\n",
    "    \"HBo7\": 130,\n",
    "    \"HBsus2\": 131,\n",
    "    \"HBsus4\": 132,\n",
    "    \"HNA\": 133\n",
    "}\n",
    "\n",
    "for i in init_dict.keys():\n",
    "    init_dict[i] = 0\n",
    "\n",
    "print(init_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_vocabs = list(init_dict.keys())\n",
    "print(init_vocabs)\n",
    "print(len(init_vocabs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_memory = {}\n",
    "for c_seq in tqdm(chord_seq):\n",
    "    for idx in range(1,len(c_seq)):\n",
    "        adj_chord = c_seq[idx-1] + c_seq[idx]\n",
    "        if adj_chord in bpe_memory:\n",
    "            bpe_memory[adj_chord] += 1\n",
    "        else:\n",
    "            bpe_memory[adj_chord] = 1\n",
    "bpe_memory = dict(sorted(bpe_memory.items(), key=lambda item: item[1], reverse=True))\n",
    "print(bpe_memory)\n",
    "print(list(bpe_memory.keys())[0])\n",
    "\n",
    "new_vocab = list(bpe_memory.keys())[0]\n",
    "\n",
    "if new_vocab in init_vocabs:\n",
    "    print(\"ERROR EXIST\")\n",
    "else:\n",
    "    init_vocabs.append(new_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(init_vocabs)\n",
    "print(len(init_vocabs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge exist sequence with new vocab\n",
    "print(chord_seq[0])\n",
    "update_data = []\n",
    "for c_seq in chord_seq:\n",
    "    update_chord = []\n",
    "    idx = 1\n",
    "    while(idx < len(c_seq)):\n",
    "        before = c_seq[idx-1]\n",
    "        current = c_seq[idx]\n",
    "        \n",
    "        if before + current == new_vocab:\n",
    "            # print(\"MERGED\")\n",
    "            update_chord.append(before+current)\n",
    "            idx += 2\n",
    "        else:\n",
    "            update_chord.append(before)\n",
    "            idx += 1\n",
    "            \n",
    "        if idx == len(c_seq):\n",
    "            update_chord.append(c_seq[idx-1])\n",
    "            break\n",
    "        if idx == len(c_seq)+1:\n",
    "            break\n",
    "    update_data.append(update_chord)\n",
    "    \n",
    "print(update_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_memory = {}\n",
    "chord_seq = update_data\n",
    "for c_seq in tqdm(chord_seq):\n",
    "    for idx in range(1,len(c_seq)):\n",
    "        adj_chord = c_seq[idx-1] + c_seq[idx]\n",
    "        if adj_chord in bpe_memory:\n",
    "            bpe_memory[adj_chord] += 1\n",
    "        else:\n",
    "            bpe_memory[adj_chord] = 1\n",
    "bpe_memory = dict(sorted(bpe_memory.items(), key=lambda item: item[1], reverse=True))\n",
    "print(bpe_memory)\n",
    "print(list(bpe_memory.keys())[0])\n",
    "\n",
    "new_vocab = list(bpe_memory.keys())[0]\n",
    "\n",
    "if new_vocab in init_vocabs:\n",
    "    print(\"ERROR EXIST\")\n",
    "else:\n",
    "    init_vocabs.append(new_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(init_vocabs)\n",
    "print(len(init_vocabs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ['a','a','a','b','a','b','a','a','c','a','a','a','b','a']\n",
    "new_test = []\n",
    "idx = 1\n",
    "\n",
    "while(idx < len(test)):\n",
    "    before = test[idx-1]\n",
    "    current = test[idx]\n",
    "    \n",
    "    if before + current == 'ab':\n",
    "        new_test.append(before+current)\n",
    "        idx += 2\n",
    "    else:\n",
    "        new_test.append(before)\n",
    "        idx += 1\n",
    "        \n",
    "    if idx == len(test):\n",
    "        new_test.append(test[idx-1])\n",
    "        break\n",
    "    if idx == len(test)+1:\n",
    "        break\n",
    "print(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_dict = Di\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, vocab):\n",
    "    bigram = ' '.join(pair)\n",
    "    replacement = ''.join(pair)\n",
    "    new_vocab = {}\n",
    "    for word in vocab:\n",
    "        new_word = word.replace(bigram, replacement)\n",
    "        new_vocab[new_word] = vocab[word]\n",
    "    return new_vocab\n",
    "\n",
    "# Function to update vocabulary with new tokens\n",
    "def update_vocab(vocab_dict, new_tokens):\n",
    "    max_key = max(int(key) for key in vocab_dict.keys())\n",
    "    for token in new_tokens:\n",
    "        if token not in vocab_dict.values():\n",
    "            max_key += 1\n",
    "            vocab_dict[str(max_key)] = token\n",
    "    return vocab_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sequences\n",
    "sequences = flat_seq\n",
    "\n",
    "# Create a frequency dictionary from sequences\n",
    "freq_dict = Counter()\n",
    "for seq in sequences:\n",
    "    for chord in seq:\n",
    "        # Split chord into characters with spaces\n",
    "        split_chord = ' '.join(list(chord))\n",
    "        freq_dict[split_chord] += 1\n",
    "\n",
    "# Apply BPE\n",
    "num_merges = 20  # Number of merges you want to perform\n",
    "for i in tqdm(range(num_merges)):\n",
    "    pairs = get_stats(freq_dict)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    freq_dict = merge_vocab(best, freq_dict)\n",
    "\n",
    "# Extract new tokens from the merged vocabulary\n",
    "new_tokens = set()\n",
    "for chord in freq_dict.keys():\n",
    "    new_tokens.update(chord.split())\n",
    "\n",
    "# Update the initial vocabulary with new tokens\n",
    "vocab = update_vocab(vocab, new_tokens)\n",
    "# print(vocab)\n",
    "\n",
    "# Save the updated vocabulary back to a file\n",
    "# with open('updated_chord.json', 'w') as f:\n",
    "#     json.dump(vocab, f, indent=4)\n",
    "\n",
    "print(\"Updated Vocabulary:\", vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_new_vocab(sequence, vocab):\n",
    "    \"\"\"Map the original sequence to the new BPE vocabulary indices.\"\"\"\n",
    "    print(\"Seq\")\n",
    "    print(len(sequence))\n",
    "    print(sequence[0])\n",
    "    print(\"Vocab\")\n",
    "    print(len(vocab))\n",
    "    print(type(vocab))\n",
    "    print(vocab)\n",
    "    print(\"T to Idx\")\n",
    "    token_to_index = {token: idx for idx, token in enumerate(vocab)}\n",
    "\n",
    "    # print(token_to_index)\n",
    "    print(token_to_index)\n",
    "    mapped_sequences = []\n",
    "    \n",
    "    for seq in sequence:\n",
    "        print(\"Inside for\")\n",
    "        print(seq)\n",
    "        mapped_seq = [token_to_index[token] for token in seq]\n",
    "        mapped_sequences.append(mapped_seq)\n",
    "    return mapped_sequences, token_to_index\n",
    "\n",
    "\n",
    "mapped_sequences, token_to_index = map_to_new_vocab(encoded_sequences, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'HC/o7': 0, 'HB/o7': 1, 'HBsus4': 2}\n",
    "b = {'asdf': 2, 'ad': 5}\n",
    "c = a|b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the desired JSON format\n",
    "index_to_token = {idx: f\"CHORD{idx}\" for idx, token in enumerate(vocab)}\n",
    "token_to_index = {f\"CHORD{idx}\": idx for idx, token in enumerate(vocab)}\n",
    "combined_vocab = {**token_to_index, **index_to_token}\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"/workspace/data/vocabs/bpe_chord_vocabs.json\", \"w\") as f:\n",
    "    json.dump(combined_vocab, f, indent=4)\n",
    "\n",
    "print(\"\\nCombined Vocabulary JSON:\")\n",
    "print(json.dumps(combined_vocab, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dicts_remove_duplicates(dict1, dict2):\n",
    "    # Combine the dictionaries\n",
    "    combined_dict = {**dict1, **dict2}\n",
    "    \n",
    "    # Identify duplicates\n",
    "    duplicates = set(dict1.items()) & set(dict2.items())\n",
    "    \n",
    "    # Remove duplicates from the combined dictionary\n",
    "    for k, v in duplicates:\n",
    "        if k in combined_dict and combined_dict[k] == v:\n",
    "            del combined_dict[k]\n",
    "    \n",
    "    return combined_dict\n",
    "\n",
    "dict1 = {'a': 1, 'b': 7, 'c': 3}\n",
    "dict2 = {'b': 2, 'c': 4, 'd': 5}\n",
    "\n",
    "combined_dict = combine_dicts_remove_duplicates(dict1, dict2)\n",
    "print(combined_dict)\n",
    "# Output: {'a': 1, 'c': 4, 'd': 5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_path = '/workspace/pj/exp/bpe_chord_vocab_1000.json'\n",
    "with open(bpe_path, 'r') as file:\n",
    "    bpe_vocab = json.load(file)\n",
    "print(bpe_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bpe_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create VOCSB Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary JSON file created successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the list of tokens\n",
    "bpe_path = '/workspace/pj/exp/bpe_chord_vocab_20000.json'\n",
    "with open(bpe_path, 'r') as file:\n",
    "    tokens = json.load(file)\n",
    "tokens = tokens[:1000]\n",
    "\n",
    "# Create a dictionary mapping each token to its index\n",
    "vocab = {token: index for index, token in enumerate(tokens)}\n",
    "\n",
    "# Add the reverse mapping from index to token\n",
    "vocab.update({index: token for index, token in enumerate(tokens)})\n",
    "\n",
    "# Save the dictionary as a JSON file\n",
    "with open('/workspace/pj/data/vocabs/chord_bpe_1000.json', 'w') as json_file:\n",
    "    json.dump(vocab, json_file, indent=4)\n",
    "\n",
    "print(\"Vocabulary JSON file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
