{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import setproctitle\n",
    "\n",
    "setproctitle.setproctitle('exp/inst/tokenizing.ipynb')\n",
    "\n",
    "class InstDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        inst_vocab_path = '/workspace/pj/data/vocabs/inst.json'\n",
    "        chord_vocab_path = '/workspace/pj/data/vocabs/chord.json'\n",
    "        with open(inst_vocab_path, 'r') as file:\n",
    "            self.inst_vocab = json.load(file)\n",
    "        with open(chord_vocab_path, 'r') as file:\n",
    "            self.chord_vocab = json.load(file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_seq = self.data[idx]\n",
    "        \n",
    "        if isinstance(text_seq, str):\n",
    "            toks = text_seq.split()\n",
    "            \n",
    "        l_toks = len(toks)\n",
    "        ratio = 4\n",
    "        chord_list = []\n",
    "        inst_in_measure = []\n",
    "        inst_list = []\n",
    "        \n",
    "        for idx in range(0, l_toks, ratio):\n",
    "            t1, t2, t3, t4 = toks[idx : idx + 4]\n",
    "            if t1[0] == 'H':\n",
    "                chord_list.append(t1)\n",
    "\n",
    "            if t4[0] == 'x' or t4[0] == 'X' or t4[0] == 'y' or t4 == '<unk>':\n",
    "                inst_in_measure.append(t4)\n",
    "                \n",
    "            if (t1[0] == 'm' or t1[0] == 'M') and len(chord_list) > 0:\n",
    "                inst_list.append(inst_in_measure)\n",
    "                inst_in_measure = []\n",
    "        inst_list.append(inst_in_measure)\n",
    "        \n",
    "        chord_tensor = [self.chord_vocab[chd] for chd in chord_list]\n",
    "        # inst_tensor, length = self.convert_inst_to_onehot(inst_list)\n",
    "        \n",
    "        target_chord_tensor = [2] + chord_tensor[:766] + [1]\n",
    "        target_chord_tensor = torch.tensor(target_chord_tensor)\n",
    "        \n",
    "        # target_inst_tensor = inst_tensor\n",
    "\n",
    "        return target_chord_tensor, inst_list\n",
    "    \n",
    "    def convert_inst_to_onehot(self, inst_list):\n",
    "        base_tensor = torch.zeros(len(inst_list), 133)\n",
    "        bos_tensor = torch.zeros(1, 133)\n",
    "        eos_tensor = torch.zeros(1, 133)\n",
    "        bos_tensor[:,2] = 1\n",
    "        eos_tensor[:,1] = 1\n",
    "        \n",
    "        for idx, inst_in_measure in enumerate(inst_list):\n",
    "            if len(inst_in_measure) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                for inst in inst_in_measure:\n",
    "                    base_tensor[idx, self.inst_vocab[inst]] = 1\n",
    "        inst_tensor = torch.cat((bos_tensor,base_tensor[:766,:],eos_tensor), dim=0)\n",
    "        return inst_tensor, len(inst_list)\n",
    "    \n",
    "def create_dataloaders(batch_size):\n",
    "    raw_data_path = '../../../workspace/pj/data/corpus/raw_corpus_bpe.txt'\n",
    "    # raw_data_path = '../../../workspace/data/corpus/first_5_lines_bpe.txt'\n",
    "    raw_data = []\n",
    "    with open(raw_data_path, 'r') as f:\n",
    "        for line in tqdm(f, desc=\"reading original txt file...\"):\n",
    "            raw_data.append(line.strip())\n",
    "            \n",
    "    train, val_test = train_test_split(raw_data, test_size=0.1, random_state=5)\n",
    "    val, test = train_test_split(val_test, test_size=0.2, random_state=5)\n",
    "    # train, val_test = train_test_split(raw_data, test_size=0.5, random_state=5)\n",
    "    # val, test = train_test_split(val_test, test_size=0.2)\n",
    "    \n",
    "    train_dataset = InstDataset(train)\n",
    "    val_dataset = InstDataset(val)\n",
    "    test_dataset = InstDataset(test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_batch)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_batch)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_batch)\n",
    "\n",
    "    # return train_loader, True, True\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def collate_batch(batch):\n",
    "    chords, insts = zip(*batch)\n",
    "    # padding_value = <eos>\n",
    "    chord_padded = pad_sequence(chords, padding_value=0, batch_first=True)\n",
    "    return chord_padded, insts\n",
    "\n",
    "def create_C2I(batch_size):\n",
    "    raw_data_path = '../../../workspace/pj/data/corpus/raw_corpus_bpe.txt'\n",
    "    # raw_data_path = '../../../workspace/data/corpus/first_5_lines_bpe.txt'\n",
    "    raw_data = []\n",
    "    with open(raw_data_path, 'r') as f:\n",
    "        for line in tqdm(f, desc=\"reading original txt file...\"):\n",
    "            raw_data.append(line.strip())\n",
    "            \n",
    "    train, val_test = train_test_split(raw_data, test_size=0.1, random_state=5)\n",
    "    val, test = train_test_split(val_test, test_size=0.2, random_state=5)\n",
    "    \n",
    "    train_dataset = C2IDataset(train)\n",
    "    val_dataset = C2IDataset(val)\n",
    "    test_dataset = C2IDataset(test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_batch_C2I)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_batch_C2I)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_batch_C2I)\n",
    "\n",
    "    # return train_loader, True, True\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def collate_batch_C2I(batch):\n",
    "    chords, insts, length = zip(*batch)\n",
    "    # padding_value = <eos>\n",
    "    chord_padded = pad_sequence(chords, padding_value=0, batch_first=True)\n",
    "    inst_padded = pad_sequence(insts, padding_value=0, batch_first=True)\n",
    "    return chord_padded, inst_padded, length\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading original txt file...: 46188it [00:18, 2522.20it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = create_dataloaders(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_dict = {}\n",
    "cnt = 0\n",
    "for (chords, targets) in tqdm(train_loader, ncols=60):\n",
    "    # unbatch\n",
    "    targets = targets[0]\n",
    "    \n",
    "    for inst_list in targets:\n",
    "        group_inst = ''\n",
    "        # inst_list = sorted(inst_list, reverse=True)\n",
    "        for inst in inst_list:\n",
    "            if inst in group_inst:\n",
    "                pass\n",
    "            else:\n",
    "                group_inst += inst\n",
    "        cnt += 1\n",
    "        \n",
    "        if group_inst not in inst_dict.keys():\n",
    "            inst_dict[group_inst] = 1\n",
    "        else:\n",
    "            inst_dict[group_inst] += 1\n",
    "            \n",
    "print(inst_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inst_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "sorted_dict = sorted(inst_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "sorted_dict = dict(sorted_dict)\n",
    "\n",
    "with open('/workspace/pj/exp/inst/sort_tokenizing.json', 'w') as json_file:\n",
    "    json.dump(sorted_dict, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████| 3695/3695 [01:11<00:00, 51.89it/s]\n"
     ]
    }
   ],
   "source": [
    "for (chords, targets) in tqdm(val_loader, ncols=60):\n",
    "    # unbatch\n",
    "    targets = targets[0]\n",
    "    \n",
    "    for inst_list in targets:\n",
    "        group_inst = ''\n",
    "        # inst_list = sorted(inst_list, reverse=True)\n",
    "        for inst in inst_list:\n",
    "            if inst in group_inst:\n",
    "                pass\n",
    "            else:\n",
    "                group_inst += inst\n",
    "        cnt += 1\n",
    "        \n",
    "        if group_inst not in inst_dict.keys():\n",
    "            inst_dict[group_inst] = 1\n",
    "        else:\n",
    "            inst_dict[group_inst] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████| 924/924 [00:15<00:00, 59.53it/s]\n"
     ]
    }
   ],
   "source": [
    "for (chords, targets) in tqdm(test_loader, ncols=60):\n",
    "    # unbatch\n",
    "    targets = targets[0]\n",
    "    \n",
    "    for inst_list in targets:\n",
    "        group_inst = ''\n",
    "        # inst_list = sorted(inst_list, reverse=True)\n",
    "        for inst in inst_list:\n",
    "            if inst in group_inst:\n",
    "                pass\n",
    "            else:\n",
    "                group_inst += inst\n",
    "        cnt += 1\n",
    "        \n",
    "        if group_inst not in inst_dict.keys():\n",
    "            inst_dict[group_inst] = 1\n",
    "        else:\n",
    "            inst_dict[group_inst] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "sorted_dict = sorted(inst_dict.items(), key=lambda item: len(item[0]))\n",
    "sorted_dict = dict(sorted_dict)\n",
    "\n",
    "with open('/workspace/pj/exp/inst/LENsort_tokenizing.json', 'w') as json_file:\n",
    "    json.dump(sorted_dict, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5992500\n"
     ]
    }
   ],
   "source": [
    "appear = 0\n",
    "\n",
    "for i in inst_dict:\n",
    "    appear += inst_dict[i]\n",
    "print(appear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4194755\n",
      "128542\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "sorted_dict = sorted(inst_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "sorted_dict = dict(sorted_dict)\n",
    "\n",
    "th = 0\n",
    "for idx, s in enumerate(sorted_dict):\n",
    "    th += sorted_dict[s]\n",
    "    if th > ((appear//10)*7):\n",
    "        print(th)\n",
    "        print(idx)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3595514\n",
      "68213\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "sorted_dict = sorted(inst_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "sorted_dict = dict(sorted_dict)\n",
    "\n",
    "with open('/workspace/pj/exp/inst/ALL_tokenizing.json', 'w') as json_file:\n",
    "    json.dump(sorted_dict, json_file, indent=4)\n",
    "\n",
    "th = 0\n",
    "for idx, s in enumerate(sorted_dict):\n",
    "    th += sorted_dict[s]\n",
    "    if th > ((appear//10)*6):\n",
    "        print(th)\n",
    "        print(idx)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {}\n",
    "\n",
    "sorted_dict = sorted(inst_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "sorted_dict = dict(sorted_dict)\n",
    "\n",
    "vocab_dict['<pad>'] = 0\n",
    "vocab_dict['<eos>'] = 1\n",
    "vocab_dict['<bos>'] = 2\n",
    "vocab_dict['<unk>'] = 3\n",
    "\n",
    "for idx, s in enumerate(sorted_dict):\n",
    "    vocab_dict[s] = idx+4\n",
    "    \n",
    "    if idx == 128542:\n",
    "        break\n",
    "    \n",
    "with open('/workspace/pj/data/vocabs/inst_group70_vocab.json', 'w') as json_file:\n",
    "    json.dump(vocab_dict, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
